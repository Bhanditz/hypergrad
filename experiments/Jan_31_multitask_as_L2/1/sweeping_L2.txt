. . . . . . . . {'N_layers': 2, 'OrderedDict': <class 'collections.OrderedDict'>, 'rms_prop': <function rms_prop at 0x7fc63ac41aa0>, 'it': <module 'itertools' (built-in)>, 'seed': 1, 'alpha': 1.0, 'partial': <type 'functools.partial'>, 'sgd': <function differentiable_fun at 0x7fc63ac418c0>, 'N_scripts_per_iter': 1, 'dictslice': <function dictslice at 0x7fc635c55140>, 'N_iters': 100, 'kylist': <function differentiable_fun at 0x7fc638ede050>, 'plot': <function plot at 0x7fc63ac41c80>, 'omniglot': <module 'hypergrad.omniglot' from '/home/dougal/Dropbox/proj/hypergrad/hypergrad/omniglot.pyc'>, '__package__': None, 'RandomState': <class 'hypergrad.util.RandomState'>, 'make_nn_funs': <function make_nn_funs at 0x7fc63331daa0>, 'np': <module 'numpy' from '/usr/local/lib/python2.7/dist-packages/numpy/__init__.pyc'>, 'initialization_scale': 0.1, '__doc__': None, 'defaultdict': <type 'collections.defaultdict'>, 'log_L2_init': -3.0, 'run': <function run at 0x7fc63ac41c08>, 'getval': <function getval at 0x7fc638ec7050>, '__builtins__': <module '__builtin__' (built-in)>, '__file__': 'experiment.py', 'batch_size': 200, 'results': ['log_L2_init = -9.0   train_loss: 0.0111627426997, validation_loss 1.08613017363', 'log_L2_init = -8.0   train_loss: 0.0133036448208, validation_loss 1.07411464547', 'log_L2_init = -7.0   train_loss: 0.0210162260279, validation_loss 1.05312678623', 'log_L2_init = -6.0   train_loss: 0.0578908743917, validation_loss 1.0676270443', 'log_L2_init = -5.0   train_loss: 0.161354961985, validation_loss 1.14886220453', 'log_L2_init = -4.0   train_loss: 0.441276235145, validation_loss 1.28216494898', 'log_L2_init = -3.0   train_loss: 1.37945399785, validation_loss 1.87066605394', 'log_L2_init = -2.0   train_loss: 3.39276559196, validation_loss 3.47556252112'], 'N_scripts': 1, 'pickle': <module 'pickle' from '/usr/lib/python2.7/pickle.pyc'>, '__name__': '__main__', 'layer_sizes': [784, 100, 55], 'VectorParser': <class 'hypergrad.nn_utils.VectorParser'>, 'beta': 0.9, 'script_corr': 0.0, 'grad': <function grad at 0x7fc638f368c0>}
--------------------------------------------------------------------------------
log_L2_init = -9.0   train_loss: 0.0111627426997, validation_loss 1.08613017363
log_L2_init = -8.0   train_loss: 0.0133036448208, validation_loss 1.07411464547
log_L2_init = -7.0   train_loss: 0.0210162260279, validation_loss 1.05312678623
log_L2_init = -6.0   train_loss: 0.0578908743917, validation_loss 1.0676270443
log_L2_init = -5.0   train_loss: 0.161354961985, validation_loss 1.14886220453
log_L2_init = -4.0   train_loss: 0.441276235145, validation_loss 1.28216494898
log_L2_init = -3.0   train_loss: 1.37945399785, validation_loss 1.87066605394
log_L2_init = -2.0   train_loss: 3.39276559196, validation_loss 3.47556252112
