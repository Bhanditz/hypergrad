. . . . . . . . {'N_layers': 2, 'OrderedDict': <class 'collections.OrderedDict'>, 'rms_prop': <function rms_prop at 0x7fe0f8673aa0>, 'it': <module 'itertools' (built-in)>, 'seed': 2, 'alpha': 1.0, 'partial': <type 'functools.partial'>, 'sgd': <function differentiable_fun at 0x7fe0f86738c0>, 'N_scripts_per_iter': 1, 'dictslice': <function dictslice at 0x7fe0f3687140>, 'N_iters': 100, 'kylist': <function differentiable_fun at 0x7fe0f6910050>, 'plot': <function plot at 0x7fe0f8673c80>, 'omniglot': <module 'hypergrad.omniglot' from '/home/dougal/Dropbox/proj/hypergrad/hypergrad/omniglot.pyc'>, '__package__': None, 'RandomState': <class 'hypergrad.util.RandomState'>, 'make_nn_funs': <function make_nn_funs at 0x7fe0f0d66aa0>, 'np': <module 'numpy' from '/usr/local/lib/python2.7/dist-packages/numpy/__init__.pyc'>, 'initialization_scale': 0.1, '__doc__': None, 'defaultdict': <type 'collections.defaultdict'>, 'log_L2_init': -3.0, 'run': <function run at 0x7fe0f8673c08>, 'getval': <function getval at 0x7fe0f68f9050>, '__builtins__': <module '__builtin__' (built-in)>, '__file__': 'experiment.py', 'batch_size': 200, 'results': ['log_L2_init = -9.0   train_loss: 0.131963307332, validation_loss 2.51759375125', 'log_L2_init = -8.0   train_loss: 0.143203886443, validation_loss 2.48592874499', 'log_L2_init = -7.0   train_loss: 0.176977662274, validation_loss 2.41171818276', 'log_L2_init = -6.0   train_loss: 0.290189693245, validation_loss 2.28008531551', 'log_L2_init = -5.0   train_loss: 0.680989503394, validation_loss 2.18319300586', 'log_L2_init = -4.0   train_loss: 1.75158824722, validation_loss 2.44004236666', 'log_L2_init = -3.0   train_loss: 3.5083173464, validation_loss 3.58986167355', 'log_L2_init = -2.0   train_loss: 3.98640653479, validation_loss 3.9864086055'], 'N_scripts': 1, 'pickle': <module 'pickle' from '/usr/lib/python2.7/pickle.pyc'>, '__name__': '__main__', 'layer_sizes': [784, 100, 55], 'VectorParser': <class 'hypergrad.nn_utils.VectorParser'>, 'beta': 0.9, 'script_corr': 0.0, 'grad': <function grad at 0x7fe0f69688c0>}
--------------------------------------------------------------------------------
log_L2_init = -9.0   train_loss: 0.131963307332, validation_loss 2.51759375125
log_L2_init = -8.0   train_loss: 0.143203886443, validation_loss 2.48592874499
log_L2_init = -7.0   train_loss: 0.176977662274, validation_loss 2.41171818276
log_L2_init = -6.0   train_loss: 0.290189693245, validation_loss 2.28008531551
log_L2_init = -5.0   train_loss: 0.680989503394, validation_loss 2.18319300586
log_L2_init = -4.0   train_loss: 1.75158824722, validation_loss 2.44004236666
log_L2_init = -3.0   train_loss: 3.5083173464, validation_loss 3.58986167355
log_L2_init = -2.0   train_loss: 3.98640653479, validation_loss 3.9864086055
