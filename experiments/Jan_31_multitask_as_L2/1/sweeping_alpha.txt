. . . . . . . . {'N_layers': 2, 'OrderedDict': <class 'collections.OrderedDict'>, 'rms_prop': <function rms_prop at 0x7f3e364a9b18>, 'it': <module 'itertools' (built-in)>, 'seed': 2, 'alpha': 1.0, 'partial': <type 'functools.partial'>, 'sgd': <function differentiable_fun at 0x7f3e364a9938>, 'N_scripts_per_iter': 1, 'dictslice': <function dictslice at 0x7f3e314bd1b8>, 'N_iters': 100, 'kylist': <function differentiable_fun at 0x7f3e347460c8>, 'plot': <function plot at 0x7f3e364a9cf8>, 'omniglot': <module 'hypergrad.omniglot' from '/home/dougal/Dropbox/proj/hypergrad/hypergrad/omniglot.pyc'>, '__package__': None, 'RandomState': <class 'hypergrad.util.RandomState'>, 'make_nn_funs': <function make_nn_funs at 0x7f3e2eb7ab18>, 'np': <module 'numpy' from '/usr/local/lib/python2.7/dist-packages/numpy/__init__.pyc'>, '__doc__': None, 'defaultdict': <type 'collections.defaultdict'>, 'log_L2_init': -4.0, 'run': <function run at 0x7f3e364a9c80>, 'getval': <function getval at 0x7f3e3472f0c8>, '__builtins__': <module '__builtin__' (built-in)>, '__file__': 'experiment.py', 'batch_size': 200, 'results': ['alpha = 0.135335283237   train_loss: 2.41417164991, validation_loss 3.16158403789', 'alpha = 0.367879441171   train_loss: 1.91500659744, validation_loss 2.69579443582', 'alpha = 1.0   train_loss: 1.73718908058, validation_loss 2.43952922705', 'alpha = 2.71828182846   train_loss: 1.73124199753, validation_loss 2.43240609897', 'alpha = 7.38905609893   train_loss: 1.77631040098, validation_loss 2.45757415246', 'alpha = 20.0855369232   train_loss: 1.9008664019, validation_loss 2.52786412874', 'alpha = 54.5981500331   train_loss: 2.23461899034, validation_loss 3.01162586207', 'alpha = 148.413159103   train_loss: 78.3257809579, validation_loss 77.8027700728'], 'N_scripts': 1, 'pickle': <module 'pickle' from '/usr/lib/python2.7/pickle.pyc'>, '__name__': '__main__', 'log_initialization_scale': -2.0, 'layer_sizes': [784, 100, 55], 'VectorParser': <class 'hypergrad.nn_utils.VectorParser'>, 'beta': 0.9, 'script_corr': 0.0, 'grad': <function grad at 0x7f3e3479e938>}
--------------------------------------------------------------------------------
alpha = 0.135335283237   train_loss: 2.41417164991, validation_loss 3.16158403789
alpha = 0.367879441171   train_loss: 1.91500659744, validation_loss 2.69579443582
alpha = 1.0   train_loss: 1.73718908058, validation_loss 2.43952922705
alpha = 2.71828182846   train_loss: 1.73124199753, validation_loss 2.43240609897
alpha = 7.38905609893   train_loss: 1.77631040098, validation_loss 2.45757415246
alpha = 20.0855369232   train_loss: 1.9008664019, validation_loss 2.52786412874
alpha = 54.5981500331   train_loss: 2.23461899034, validation_loss 3.01162586207
alpha = 148.413159103   train_loss: 78.3257809579, validation_loss 77.8027700728
