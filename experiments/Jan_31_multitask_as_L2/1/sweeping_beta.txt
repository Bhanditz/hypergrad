. . . . . . . . {'N_layers': 2, 'OrderedDict': <class 'collections.OrderedDict'>, 'rms_prop': <function rms_prop at 0x7f4ba3a44b18>, 'it': <module 'itertools' (built-in)>, 'seed': 2, 'alpha': 1.0, 'partial': <type 'functools.partial'>, 'sgd': <function differentiable_fun at 0x7f4ba3a44938>, 'N_scripts_per_iter': 1, 'dictslice': <function dictslice at 0x7f4b9ea591b8>, 'N_iters': 100, 'kylist': <function differentiable_fun at 0x7f4ba1ce20c8>, 'plot': <function plot at 0x7f4ba3a44cf8>, 'omniglot': <module 'hypergrad.omniglot' from '/home/dougal/Dropbox/proj/hypergrad/hypergrad/omniglot.pyc'>, '__package__': None, 'RandomState': <class 'hypergrad.util.RandomState'>, 'make_nn_funs': <function make_nn_funs at 0x7f4b9c135b18>, 'np': <module 'numpy' from '/usr/local/lib/python2.7/dist-packages/numpy/__init__.pyc'>, '__doc__': None, 'defaultdict': <type 'collections.defaultdict'>, 'log_L2_init': -4.0, 'run': <function run at 0x7f4ba3a44c80>, 'getval': <function getval at 0x7f4ba1ccb0c8>, '__builtins__': <module '__builtin__' (built-in)>, '__file__': 'experiment.py', 'batch_size': 200, 'results': ['beta = 0.993262053001   train_loss: 2.1302988124, validation_loss 2.76463817427', 'beta = 0.988068477464   train_loss: 1.9799739287, validation_loss 2.54590667844', 'beta = 0.978871720119   train_loss: 1.71387408523, validation_loss 2.41182068734', 'beta = 0.962586148633   train_loss: 1.70808561244, validation_loss 2.42667597071', 'beta = 0.933747740848   train_loss: 1.73489655691, validation_loss 2.43371864348', 'beta = 0.882680833906   train_loss: 1.73779661756, validation_loss 2.44103868139', 'beta = 0.792251812856   train_loss: 1.73932223458, validation_loss 2.44742176522', 'beta = 0.632120558829   train_loss: 1.73977848843, validation_loss 2.45036441934'], 'N_scripts': 1, 'pickle': <module 'pickle' from '/usr/lib/python2.7/pickle.pyc'>, '__name__': '__main__', 'log_initialization_scale': -2.0, 'layer_sizes': [784, 100, 55], 'VectorParser': <class 'hypergrad.nn_utils.VectorParser'>, 'beta': 0.9, 'script_corr': 0.0, 'grad': <function grad at 0x7f4ba1d3a938>}
--------------------------------------------------------------------------------
beta = 0.993262053001   train_loss: 2.1302988124, validation_loss 2.76463817427
beta = 0.988068477464   train_loss: 1.9799739287, validation_loss 2.54590667844
beta = 0.978871720119   train_loss: 1.71387408523, validation_loss 2.41182068734
beta = 0.962586148633   train_loss: 1.70808561244, validation_loss 2.42667597071
beta = 0.933747740848   train_loss: 1.73489655691, validation_loss 2.43371864348
beta = 0.882680833906   train_loss: 1.73779661756, validation_loss 2.44103868139
beta = 0.792251812856   train_loss: 1.73932223458, validation_loss 2.44742176522
beta = 0.632120558829   train_loss: 1.73977848843, validation_loss 2.45036441934
