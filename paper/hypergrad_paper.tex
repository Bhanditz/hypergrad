\documentclass{article}
\usepackage{times}
\usepackage{graphicx}
\usepackage{subfigure} 
\usepackage{natbib}
\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage{algpseudocode}
\usepackage{amsmath}
\usepackage{hyperref}
\newcommand{\theHalgorithm}{\arabic{algorithm}}
%\usepackage{icml2015stylefiles/icml2015} 
\usepackage[accepted]{icml2015stylefiles/icml2015}


\newcommand{\vx}{\mathbf{x}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\vg}{\mathbf{g}}
\newcommand{\vzero}{\mathbf{0}}
\newcommand{\ones}[1]{\mat{1}_{#1}}
\newcommand{\eye}[1]{\mat{E}_{#1}}
\newcommand{\tra}{^{\mathsf{T}}}
\newcommand{\vect}[1]{{\bf{#1}}}
\newcommand{\mat}[1]{\mathbf{#1}}
\newcommand{\pderiv}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\npderiv}[2]{\nicefrac{\partial #1}{\partial #2}}

\newcommand{\numhypers}{N}
\newcommand{\hypers}{{\boldsymbol{\theta}}}
\newcommand{\params}{\vx}
\newcommand{\numsteps}{T}
\newcommand{\decay}{\gamma}
\newcommand{\decays}{{\boldsymbol{\decay}}}
\newcommand{\stepsize}{\alpha}
\newcommand{\stepsizes}{{\boldsymbol{\stepsize}}}
\newcommand{\gradparams}{\nabla_\params L(\params_t, \hypers)}
\newcommand{\gradparamst}{\nabla_\params L(\params_t, \hypers)}

\newcommand\ourtitle{Gradient-based Hyperparameter Optimization through Reversible Learning}
\icmltitlerunning{\ourtitle} % A short form for the running title.

\begin{document} 

\twocolumn[
\icmltitle{\ourtitle}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2015
% package.
\icmlauthor{Dougal Maclaurin}{maclaurin@physics.harvard.edu}
%\icmladdress{Harvard University, Cambridge, Massachusetts}
\icmlauthor{David Duvenaud}{dduvenaud@seas.harvard.edu}
%\icmladdress{Harvard University, Cambridge, Massachusetts}
\icmlauthor{Ryan P. Adams}{rpa@seas.harvard.edu}
%\icmladdress{Harvard University, Cambridge, Massachusetts}

% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{hyperparameters, neural networks, reversible computation, automatic differentiation, machine learning, ICML}

\vskip 0.3in
]

\begin{abstract} 
Tuning more than a small number of hyper parameters remains difficult due to the fact that gradients are typically unavailable.
We compute gradients of the cross-validation objective w.r.t. hyperparameters by chaining derivatives through the \emph{entire training procedure}, including hundreds of iterations of back propagation.
These gradients allows us to richly parameterize our learning procedures, optimizing complex learning rate schedules, weight initialization distributions, and neural net architectures.
We show state-of-the art results?
Our results also provide examples of optimal learning schedules and initialization strategies for several benchmark problems.
\end{abstract} 

\section{Introduction}
\label{intro}



\section{Meta-Optimization}

The current state-of-the-art for hyperparameter optimization is achieved by model-based, gradient-free methods~\cite{snoek2012practical, bergstra2011algorithms, BerYamCox13, HutHooLey11}.
These methods are able to effectively exploit the information learned through evaluations of the objective.
However, in general they are not able to effectively optimize more than 10 to 20 parameters.
Most importantly from our perspective, current hyper parameter optimization schemes do not usually have access to gradients with respect to these hyperparameters, although they could incorporate this information if it were available~\cite{solak2003derivative}.

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{../experiments/Jan_25_Figure_1/2/learning_curves.pdf}}
\caption{We backpropagate gradients of the final loss through the entire learning procedure in order to update learning and model hyperparameters.}
\label{fig:chaos}
\end{center}
\vskip -0.2in
\end{figure} 

\section{Automatic Differentiation}

Automatic differentiation (AD) software packages such as Theano~\cite{Bastien-Theano-2012, bergstra2010scipy} are a workhorse of deep learning, significantly speeding up development time by providing gradients automatically.
However, these methods are typically only used to train parameters with respect to some loss.
\citet{Autodiff14} suggest using automatic differentiation to compute gradients with respect to hyperparameters.

\subsection{Forward vs. Reverse-Mode Differentiation}

By the chain rule, the gradient of a set of nested functions is a product of the individual derivatives:
%
\begin{align*}
\pderiv{f_4(f_3(f_2(f_1(x))))}{x} = \pderiv{f_4}{f_3} \cdot \pderiv{f_3}{f_2} \cdot \pderiv{f_2}{f_1} \cdot \pderiv{f_1}{x}
\end{align*}
%
Forward-mode AD works by multiplying gradients in the same order as the functions are evaluated:
%
\begin{align*}
\pderiv{f_4(f_3(f_2(f_1(x))))}{x} = \pderiv{f_4}{f_3} \cdot \left( \pderiv{f_3}{f_2} \cdot \left( \pderiv{f_2}{f_1} \cdot \pderiv{f_1}{x} \right) \right)
\end{align*}
%
Reverse-mode AD works by maintaining a tape of computations performed, and computing the gradient in a backwards pass, starting from the final result:
%
\begin{align*}
\pderiv{f_4(f_3(f_2(f_1(x))))}{x} = \left(  \left(  \pderiv{f_4}{f_3} \cdot \pderiv{f_3}{f_2} \right) \cdot \pderiv{f_2}{f_1} \right) \cdot \pderiv{f_1}{x} 
\end{align*}
%
If each function has multivariate inputs and outputs, the gradients can be replaced with Jacobian matrices, but the order or operations remains the same.

\textbf{Computational Complexity}
In forward-mode, gradients can be computed at the same time as the original function, by augmenting the forward computation with the directional gradient.
This method has only twice the memory cost of simply computing the function of interest, but in general requires a separate forward pass for each input parameter, resulting in an $\mathcal{O}(\numhypers)$ slowdown~\cite{pearlmutter2008reverse}.
Naive reverse-mode AD typically only requires approximately twice the time as computing the original function, but can require up to $\mathcal{O}(\numsteps\numhypers)$ memory.
In the next sections, we show how to drastically reduce the memory requirements of reverse-mode AD for differentiating through the entire learning procedure.

\section{Reversible Learning}

In the case of large neural networks, the amount of memory required to store the millions of parameters being trained is typically close to the amount of physical RAM availabe~\cite{sequence2014}.
Thus storing every parameter value throughout training in order to perform reverse-mode differentiation is completely impractical.
If storing the parameter vector takes $\sim$1GB, and the parameter vector is updated tens of thousands of times (equal to the number of mini batches times the number of epochs) then storing the learning history becomes unmanageable even in physical storage.

This section discusses how we overcome the technical difficulty of recovering the history of parameter vectors in order to perform reverse-mode AD, when performing stochastic gradient descent with momentum.

\subsection{Reversible Dynamics}

Stochastic gradient descent with momentum (Algorithm \ref{alg:sgd}) can be seen as a physical simulation of a system moving through a fixed force field.
%
\begin{algorithm}
   \caption{Gradient Descent with Momentum}
   \label{alg:sgd}
\begin{algorithmic}[1]
   \State {\bfseries input:} initial $\vx_1$, decays $\decays$, stepsizes $\stepsizes$, loss $L(\params, \hypers)$
   \State initialize $\vv_1 = \vzero$
   \For{$t=1$ {\bfseries to} $T$}
   \State $g_t = \gradparamst$ \Comment{evaluate gradient}
   \State $\vv_{t+1} = \decay_t \vv_t + (1 - \decay_t) \vg_t$ \Comment{update velocity}
   \State $\vx_{t+1} = \vx_t + \stepsize_t \vv_t$ \Comment{update position}
   \EndFor
   \State \textbf{output} trained parameters $\vx_T$
\end{algorithmic}
\end{algorithm}
%
We can also view stochastic gradient descent as a function, which takes an initial parameter vector, a step-size schedule, and a learning rate schedule.
How to take the gradient of SGD with respect to these parameters?

If the momentum decay term $\gamma$ is set to one, then we recover a dynamics scheme known as the leapfrog, or Verlet, integrator~\citep{leapfrog1995}.
These dynamics are exactly reversible through time.
Exactly reversible dynamics such as these would allow us to trace the neural net parameter vector backwards, starting from their final value back going to the initial values.
Having the parameter vectors available in this order would allow reverse-mode AD having fixed memory requirements no matter how many iterations of learning are performed.

Even if the decay were less than one, if one could somehow simulate having exact real numbers, this procedure could be reversed, and used for reverse-mode AD.
This procedure is given by Algorithm \ref{alg:reverse-sgd}.
It is obtained by simply reversing the steps in Algorithm \ref{alg:sgd}, interleaved with computations of gradients w.r.t. the parameters of SGD, namely the initial parameters $\vx_1$, and the stepsize and momentum schedules.
%
\begin{algorithm}
   \caption{Memory-efficient Reverse-Mode \\ Differentiation through Gradient Descent with Momentum}
   \label{alg:reverse-sgd}
\begin{algorithmic}[1]
   \State {\bfseries input:} $\vx_T$, $\vv_T$, $\decays$, $\stepsizes$, train loss $L(\params, \hypers)$, function $f(\params)$
   \State initialize $d\vv = \vzero$, $d\hypers = \vzero$, $d\stepsize_t = \vzero$, $d\decay = \vzero$
   \State initialize $d\vx = \nabla_\params f(\params_T)$
   \For{$t=T$ {\bfseries counting down to} $1$}
   \State $d\stepsize_t = d\vv\tra \vv_t$
   \State $\vx_{t+1} = \vx_t - \stepsize \vv_t$
   \vspace{-0.95\baselineskip}
   \State $\vg_t = \gradparamst$
   \hfill \scalebox{1.1}{\Bigg\}} \vspace{-\baselineskip} \begin{minipage}{2.5cm} exactly reverse \\ SGD operations \\ using bit-stores \strut \end{minipage}
   \State $\vv_{t+1} = [\vv_t - (1 - \decay) \vg_t] / \decay$
   \State $d\vv = d\vv + \stepsize d\vx$
   \State $d\decay_t = d\vx\tra (\vv_t + \vg_t)$
   \State $d\vx = d\vx - (1 - \decay) d\vv \nabla_\params \gradparamst$ \label{line:hvp1}
   \State $d\hypers = d\hypers - (1 - \decay) d\vv \nabla_\hypers \gradparamst$ \label{line:hvp2}
   \State $d\vv = \decay d\vv$
   \EndFor
   \State \textbf{output} gradient of $f(\vx_T)$ w.r.t $\vx_1$, $\vv_1$, $\decays$, $\stepsizes$ and $\hypers$
\end{algorithmic}
\end{algorithm}
%

\paragraph{Computational complexity}
Computations of steps \ref{line:hvp1} and \ref{line:hvp2} requires a Hessian-vector product, which can be computed exactly in about the same time required to compute a gradient \citep{pearlmutter1994fast}.
These gradients are in principle derivable by hand, but in order for this procedure to work without extra work by the user, we would require an automatic differentiation package which could compute second derivatives.

\subsection{Self-closed Automatic Differentiation}
\citet{pearlmutter2008reverse} implemented the first example of an automatic differentiation package that was \emph{closed under its own operation}, meaning it could be used to take 2nd or 3rd gradients simply by applying the gradient operation more than once. [Add more details]
However, this package could only operate on code written in a limited form of Scheme in which each function could only have one argument.
Popular AD popular packages such as Theano do not have this ability to automatically compute higher-order gradients.
Thus, we implemented our own automatic differentiation package (available at \url{www.redacted.com}) for Python that is closed under its own operation.
This package has the additional feature that it operates on standard Numpy~\cite{oliphant2007python} code.

\subsection{The decay term causes non-reversibility}
%In practice, however, best performance is usually achieved when the decay term set to a value such as $0.98$.
The decay term must be less than one in order for the optimizer to converge.
Since information is lost each time decay is applied, and the learning process is no longer exactly reversible.
Even if double-precision floating point numbers are used, errors accumulate exponentially, and the reversed learning procedure ends far from the initial point.
This means that naively applying Algorithm \ref{alg:reverse-sgd} will not provide accurate gradients if the decay term is less than one.

\section{Reversible computation even with non-reversible dynamics}

To get around the problem of information loss caused by decay, we need to somehow store that information.
We could easily do so by storing the weight vector at each iteration, but this would require too much memory to be practical for large neural nets trained for thousands of minibatches.
Luckily, when the decay factor is close to 1, the amount of information lost at each iteration is small.

\textbf{Quantifying the Savings}
for example, an integer multiplied by 0.98 will lose $0.029$ bits of information on average.
Compared to storing a new 32-bit integer or floating-point number, 
Thus, if we needed only to store the small amount of information lost at each iteration, memory requirements would only be ${\frac{0.029}{32} = 0.1\%}$ of what they would otherwise be.
Even if the decay term is $0.5$, memory requirements are still only $\frac{1}{32}$ of what they would otherwise be.

\subsection{Storing Fractional Bits on a Tape}

for each parameter being updated during learning, we keep a tape of bits.
Each time an information-destroying operation (such as division by an integer) is performed, we store the information that would be needed to reverse the operation exactly.  In the case of division by an integer, this information is simply the remainder.
During the reverse pass, when the inverse operation is performed (multiplication by that same integer), this extra information is extracted from the tape.

[TODO: Explain fractional bits, and break up the following algorithm into multiply and divide steps.]

\begin{algorithm}
   \caption{Reversible Multiplication by a Ratio}
   \label{alg:reversible-mult}
\begin{algorithmic}[1]
   \State {\bfseries Input:} Giant integer $i$, Current value $c$, Numerator $n$, Denominator $d$
   \State $i = id$ \Comment{make room for new bits from the division}
   \State $i = i + (c \mod d)$ \Comment{store leftover bits from division}
   \State $c = c / d$ \Comment{divide by denomenator}
   \State $c = cn$ \Comment{multiply by numerator}
   \State $b = i \mod n$ \Comment{extract bits to add after multiplication}
   \State $i = i / n$ \Comment{get rid of extra room}
   \State $c = c + b$ \Comment{insert extra bits from tape}
   \State \textbf{return} new integer $i$, new value $c$
\end{algorithmic}
\end{algorithm}
%


\section{When are gradients meaningful?}

Gradient-based optimizers can perform poorly when the surface being optimized is locally ``wiggly''.
Small wiggles cause both the direction and size of the gradient to be less correlated with the medium-scale shape of the function.
When do we expect the training loss of a neural network to be wiggly with respect to hyperparameters?

\paragraph{The edge of chaos}
\citet*[Chapter 4]{pearlmutter1996investigation} discusses the chaotic behavior of gradient descent with momentum when training learning neural networks.

One problem with training these networks is that when the learning rate is too high, the gradient becomes uninformative about the medium-term behavior of the function.
Figure \ref{fig:chaos} illustrates this problem.

\cite{pearlmutter2009sleep} argues that biological neural networks tune themselves so as to be at the "edge of chaos".
Empirically, it seems like the best learning rates for training neural networks are just small enough to avoid this chaotic behavior.
Lower learning rates don't reach convergence before training ends.


\begin{figure}[h!]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{../experiments/Jan_14_learning_rate_wiggliness/1/fig.png}}
\caption{When the learning rate is too high, the gradient becomes uninformative about the medium-term behavior of the function.
The step sizes which lead to chaotic behavior are only slightly larger than the optimal step sizes.}
\label{fig:chaos}
\end{center}
\vskip -0.2in
\end{figure} 


%\begin{figure}[ht]
%\vskip 0.2in
%\begin{center}
%\centerline{\includegraphics[width=\columnwidth]{../experiments/Jan_15_optimize_learning_rate_schedule/2/fig.png}}
%\caption{Gradients with respect to step size as a function of training epoch.}
%\label{fig:chaos}
%\end{center}
%\vskip -0.2in
%\end{figure} 


\section{Results}

\subsection{Optimizing Step-size and Momentum Schedules}

Having access to gradients allows us to optimize thousands of hyperparameters.

Each primal iteration uses a different set of random initial weights, as well as a random reshuffling of dataset.
This prevents the learning schedules from depending on quirks of the initial random initialization.

\begin{figure}[h!]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{../experiments/Jan_28_training_schedule/4_training_loss_nn/alpha_beta_paper.pdf}}
\caption{\emph{Top:} A step-size training schedule optimized by hyper-gradient descent.
\emph{Bottom:} The momentum schedule optimized for the same task.}
\label{fig:optimal schedule}
\end{center}
\vskip -0.2in
\end{figure} 



\begin{figure}[h!]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{../experiments/Jan_28_training_schedule/4_training_loss_nn/scale_and_reg.png}}
\caption{\emph{Top:} Meta-training curves for the initialization scales of different types of weights.
Meta-learning sets the weight distribution of the first layer weights to be very small, and the second layer weights to be large.
Only one layer needs to be non-zero at initialization in order to break symmetry.}
\label{fig:nn meta-learning curves}
\end{center}
\vskip -0.2in
\end{figure} 

\subsubsection{Overfitting}

How many hyperparameters can we fruitfully optimize?
The main limitation seems to be overfitting.

\begin{figure}[h!]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{../experiments/Jan_28_training_schedule/4_training_loss_nn/learning_curves.png}}
\caption{\emph{Top:} Learning curves.  \emph{Middle:} Meta-learning curves. }
\label{fig:nn meta-learning curves}
\end{center}
\vskip -0.2in
\end{figure} 

Figure \ref{fig:nn meta-learning curves} shows the difference between training, validation and test-set error when optimizing 


\subsection{Learning to Learn Languages}

\begin{figure}[h!]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{../experiments/Jan_27_first_omniglot_expt/random_images.png}}
\caption{Example languages drawn from the Omniglot dataset.}
\label{fig:omniglot}
\end{center}
\vskip -0.2in
\end{figure} 




\section{Related Work}

\subsection{Gradient-Based Training of Hyperparameters}
\textbf{Neural Nets}
\citet{bengio2000gradient, larsen1998adaptive} identified the benefits of using gradients to optimize the cross-validation loss with respect to neural net hyperparameters, and showed several simple proofs-of-concept.
However, they were only able to compute gradients of hyperparameters controlling fixed, known functions of the weights, and so could not optimize parameters such as step-sizes or initialization distributions.

\textbf{Support Vector Machines}
\citet{chapelle2002choosing} point out that the lack of gradients in the support vector machine (SVM) objective limits the number of kernel parameters, usually to one or two.
They introduced a differentiable bound on the SVM loss in order to be able to compute derivatives with respect to hundreds of hyperparameters, including weighting parameters for each input dimension in the kernel.
However, this bound was not tight. [Need to have another read to figure out limitations of their approach]

\textbf{Bayesian Methods}
for Bayesian methods that have a closed-form marginal likelihood, it is already the case that gradients are available with respect to an many hyperparameters as desired.
for example, this sort of flexibility has been used to construct complex, custom kernels for Gaussian process models~\cite{rasmussen38gaussian}[Chapter 5].
%or to train the parameters of Markov random fields \cite{samuel2012gradient}
Variational inference also allow the tuning of hyperparameters in neural-network like models such as deep Gaussian processes~\citep{deepGPVar14}.
However, these methods do not enable the tuning of the parameters which themselves tune the marginal likelihood.

\subsection{Reducing the Number of Hyperparameters}
Several papers have addressed the problem of adapting learning parameters by attempting to set them automatically, or lessen sensitivity to their exact values, such as \cite{schaul2012no, Adam14, Adasecant14, Hotswap14}.
However, these methods all retain at least a small number of hyperparameters.
In principle, the method we propose here can tune any remaining parameters.
More importantly, having access to hyper-gradients might make the introduction of new, difficult-to-tune hyperparameters worthwhile.

\subsection{Gradients of Iterative Procedures}

\citet{Bridging14} tune the step-size and mass-matrix parameters of Hamiltonian Monte Carlo by chaining gradients back from a lower bound on the marginal likelihood.
Because their application required a small, fixed number of steps of relatively few parameters, they did not require the memory-tape approach presented in this paper.

\textbf{The Vanishing Gradient Problem}
Training hyperparameters whose dependence on the final objective depends on many hundreds of iterations by gradient descent raises several issues.
\citet{bengio1994learning} noted that "learning long-term dependencies with gradient descent is difficult."

Exploding-gradient problem~\cite{pascanu2012understanding}

\section{Extensions and Future Work}

What other types of computation have a similar flavour to gradient descent with momentum?
Long Short-Term Memory has a similar flavour, as do the computations performed by Neural Turing Machines.
Reversible Learning might allow gradient (not hyper-gradient) computations with respect to very long computations performed by either architecture.

\section{Conclusion}

Implementing reversible learning as presented in this paper requires either that the learning algorithm is a form of (stochastic) gradient descent with momentum, and that Hessian-vector products are available.
Even when this is not the case, we hope that the training schedules and initialization schemes demonstrated in this paper can be a guide to practitioners for training tasks that aren't amenable to this sort of procedure.

\section*{Acknowledgments} 


\bibliography{references.bib}
\bibliographystyle{icml2015stylefiles/icml2015}

\end{document} 

